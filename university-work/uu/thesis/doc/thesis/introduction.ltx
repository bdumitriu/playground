\chapter{Introduction}

\section{Motivation: towards open compilers}
\label{sec:motivation}

There are a number of aspects regarding the compilation process that
have been changing steadily over the years. Compilers have evolved
from compiling assembly code into machine code to compiling ever
higher level languages to machine-independent code (such as Java
bytecode or .NET CIL). The number of intermediate languages (and
corresponding \emph{compilation phases}) through which a program goes
has increased as well, since most modern language compilers build on
already existing ones. The common pattern is to write a compiler which
produces source code of a lower level language and then piggyback on
the compiler for that language (with C being the most widely used
one). The number of such phases increases as languages drift further
and further away from machine code in terms of abstractions.

While the above mentioned progress can be regarded as only partially
compiler-specific (with the rest spanning also language and tool
chain), there is another aspect which remains entirely within the
bounds of compiler technology: \emph{optimization}. This has been a
constant preoccupation of compiler writers from the earliest of ages.
In the beginning, most optimizations were geared towards limiting the
amount of memory needed by a program, due to the scarcity of hardware
resources. With time, the goals of optimization have widened to cover
aspects like instruction locality, computing data dependencies for
parallelization purposes, reducing the amount of branching in code,
eliminating redundancy (e.g., loop-invariant code motion) and
countless others. Nowadays a lot of work still goes into refining
existing optimizations, identifying additional optimization
opportunities and optimization algorithms, making more information
about the code available to the optimization process, extending
optimizations from basic peephole ones to whole-program ones, and so
on. Considering the scope of their application (virtually all compiled
software), it reasonable to think that advances in optimization
technology are responsible, on average, for more software quality
improvements than any other field of computer science.

Compilers have also evolved quite a lot where safety of the generated
programs is concerned.  This is achieved chiefly through advances in
\emph{type systems}. While type systems have uses beyond safety (such
as supporting optimizations or acting as documentation to a program),
it is still the case that we primarily use them for enhancing the
safety of our programs. A sufficiently advanced type system will point
out many kinds of errors in our programs at compile time, which is a
significant advantage according to the general observation that the
earlier in the development process we identify bugs, the less costly
it is to fix them. Type system have evolved along with compilers from
a rudimentary int-float-bool-char state to one including enhanced
notions like polymorphic types, generic types, dependent types or
recursive types together with various combinations and specializations
thereof. Such developments illustrate that the type systems that
compilers support today are a long way from what we have begun with in
the early days; and type system research is still receiving
significant effort.

As summarized before, aspects like varying compilation phases,
optimizations and type systems have all received proper attention both
from the research world and from the industry. Significant evolutions
in all these area stand as proof of this. But there is at least one
path related to compiler technology that has been seriously less
walked until now, namely that of \emph{open compilers}.  Open
compilers are about user customization of the compilation process. To
date, the interaction a user can have with the compiler is reduced to
specifying a number of compiler flags which trigger or prevent the
execution of specific compiler tasks. Examples of things that can be
turned on or off by the user are various types of optimizations
(including machine-dependent ones), the inclusion of debugging
information and certain types of warnings. The user cannot, however,
customize, extend or aid any of the compiler tasks, and the one we
particularly have in mind is optimization.

In general, compiling a program to provably best machine code (either
in terms or size or speed) is undecidable. In particular, even
striving for a far less ambitious goal than that still creates enough
difficulties. It is a fact that optimizers that come with today's
compilers are far from being smart enough to materialize all the
optimization potential that exists even at a (theoretically) easily
available level. This is caused primarily by the \emph{soundness}
requirement (i.e., the compiler usually has to guarantee that the
optimization is correct), which forces the optimizers to make
conservative choices (about various things) whenever there is not
enough information available. In simpler terms, optimizers are not
allowed to guess, even if there is a 99.99\% chance that the guess
will be correct.  This lack of information which optimizers are often
confronted with can have several causes. It can be that the
information is simply not there (e.g., compiled libraries for which
the source code is not available), or that the optimizer is not
``smart'' enough to extract that information even if it is available,
or that extracting the information, even if possible, would take more
time than the user is willing to allow. Whichever the cause, the
effect is the same: optimizers miss optimizations opportunities
because they have to make conservative choices.

It is because of that that the idea of open compilers was born. An
open compiler will offer the user the chance to provide that missing
information to the optimizer in an easily-accessible form.  This leads
to the realization that one of the requirements in order to write an
open compiler is that of facilitating the development of
\emph{customizable transformations} (or optimizations) so that the
user can specify this missing or hard to compute information. The kind
of information that is provided and the form in which it is given will
vary with the purpose of the optimization, but the basic idea stays
the same. The Broadway compiler \cite{guyer99annotation,
guyer-broadway}, for instance, allows the user to specify information
as trivial as the ``uses'' and ``defs'' for the parameters of a
subroutine. However trivial, this information is still very useful in
the case of missing library source code, when the compiler cannot
infer whether or not a variable will be modified or even accessed
within a routine call. As a consequence, it will always have to assume
``the worst'', namely that it is modified, to ensure correct behavior
in all cases.

Lamping et al. \cite{lamping92compilers} discuss an open compiler for
Scheme which would allow the user to make the decision of how to
implement certain variables of interest. The concept they are
promoting is to let the user benefit from all the advantages of a
high-level language like Scheme for most purposes, but still have a
way to influence the low-level representation of the runtime data
structures pointed to by these user-chosen variables, if this is what
the user needs. Kickzales et al. abstract away from this idea in a
later paper \cite{kiczales97open} where they advocate reducing the
opacity of traditionally black box software modules so that the user
has better control over the implementation that is used when more that
one is available.  They no longer discuss open compilers but
\emph{open implementations}. This idea of letting the user participate
in the selection of an implementation is not our goal per se, but it
makes for an interesting analogy. We can parallel the optimizations to
be applied to the module which used to be a black box and the user
annotations for customizing them to the mechanism for reducing the
opacity. In this way, we realize that the two are simply facets of the
same concept.

Tourw\'e and De Meuter \cite{tourwe99opt} consider a slightly more
particular case of the general problem of missing information. This
case refers to object-oriented programs, for which compilers are
having a difficult time performing virtually any type of whole-program
optimization. The impossibility arises from the dynamic dispatching of
method calls, which prevents the compiler from knowing the particular
definition of a method that a method call really refers to. The
authors give the example of inlining a method body as an optimization
which cannot be performed since it is not known at compile time which
of the potentially many bodies of a method has to be inlined. Further
on, this problem is aggravated by the fact that modern software
architectural principles promote the use of objects and methods in a
way that increases the degree of polymorphism in a program, making it
ever harder for compilers to optimize. While some researchers (such as
Dean et al. in \cite{dean95optimization}) worked out methods for
finding out the precise type an object will have at runtime (with
considerable success when this is actually possible) and thus replace
dynamic dispatch with static binding, Tourw\'e and De Meuter chose the
approach of an open compiler which the user can feed with information
through Prolog predicates-like annotations. The information given to
the compiler indicates design patterns in the source code, so that the
compiler can make architectural optimizations. Using this information
(and inferring additional one) the compiler performs a
source-to-source transformation of the code, reimplementing the entire
design pattern with different, but equivalent code such that the
resulting implementation reduces the degree of polymorphism as much as
possible. Then, it is assumed that a source-to-binary compiler will
properly identify the lack of polymorphism and perform static binding
of the method calls.

\begin{comment}
meta object protocol tools (Template Haskell, Magik, ...) ...  most
focus is on user access at runtime through reflection...  generic
programming...
\end{comment}

Steps towards creating an open compiler (for Java) have also been
taken within the context of Stratego \cite{stratego}. It is not the
case that Stratego already has a working open compiler implementation.
What it does have is a set of tools and libraries which will prove to
be extremely valuable once work on an implementation actually begins.
First of all, research on the Stratego framework has produced
significant results regarding the specification of data-flow
transformations, presented in \cite{bravo05dynrules,
  karina05dynrules}. We will cover these later, in section
\ref{ch:dynam-rules-libr} of this thesis. Secondly, Stratego
already contains two libraries called java-front \cite{javafront} and
Dryad \cite{dryad}. Java-front offers full parsing and unparsing
support for Java 5.0, which is useful both for source-to-source and
source-to-binary transformations, since we can directly manipulate an
abstract syntax tree (AST) representation of the Java classes without
having to worry about the rest. Dryad improves an already parsed Java
AST by qualifying and reclassifying names and typing variables and
expressions. In addition, Dryad also provides a Java bytecode parser
and unparser, so with a (currently unavailable) transformation from
Java AST to Java bytecode AST, we already have a basic working
compiler.

The final piece in the Stratego support for open compilers puzzle is
represented by MetaBorg. MetaBorg is a ``method for providing concrete
syntax for domain abstractions to application programmers''
\cite{metaborg}. Usually we start with a general purpose language and
extend it to support a domain specific language which abstracts over
an existing API. \cite{bravo04metaborg} discusses how Stratego can be
used to easily achieve the two steps that the method implies:
\emph{embedding} the syntax definition of the first language into the
second and \emph{assimilating} the (already parsed) representation of
one language into that of the other. A number of such embeddings have
already been developed and are presented in \cite{bravo04metaborg}:
Swul -- embedding of a DSL for building Swing user interfaces in Java,
XML in Java, Java in Java, etc. By employing SDF (Syntax Definition
Formalism) and SGLR (Scannerless Generalized LR) parser (both of them
part of the Stratego platform), MetaBorg can allow full modularization
of the language syntax definitions (thanks to the generalized LR
parser and the modularity features of SDF), can automatically generate
a disambiguating parser for various language combinations (thanks to
SDF's facilities for disambiguation: priority definitions,
associativity specifications, reject productions or follow
restrictions) and can correctly parse tokens of different languages
depending on the context in which they appear (thanks to the
scannerless parser).

The question that remains to be answered is how exactly MetaBorg fits
in the open compiler picture.  A different use of open compilers than
the one we have already discussed has to be introduced in order to
answer that. What we want is to let the user intervene in the
compilation process not only by providing extra information for aiding
compiler optimizations, but also by potentially extending the source
language of the open compiler. We clearly only expect a restricted
category of users (which we call meta programmers) to be interested in
implementing extensions to a language, but we believe that this
category is large enough to justify an effort in this direction. The
extensions we have in mind are exactly the ones that MetaBorg
facilitates: that of DSLs in general purpose languages, hence
answering the question asked at the beginning of this paragraph. With
the mechanism generically referred to as MetaBorg already available,
what is left to be done is completing the Stratego framework with the
proper technologies to support \emph{extensible transformations}. More
explicitly, we need to come up with a mechanism which would allow the
transformations defined for a language to be easily extended so that
they cover the user-defined embedding as well.

In essence, we believe that currently there is a serious need for open
compilers in the object-oriented programming languages community and
that researchers have to take this necessity into consideration. The
road that ends with providing the software world with open compilers
is long and we have merely taken a few steps on it, by exploring what
an already advanced platform like Stratego still lacks for opening the
way towards the creation of such a compiler. As explained in this
section, we believe a technology that supports \emph{extensible and
  customizable transformations} to be the core for a future open
compiler (or rather that part of the core which has not been
sufficiently explored yet). Therefore, our focus with this thesis has
been on (1) filling some of the gaps in the Stratego platform which
are necessary for and (2) somewhat experimenting with extensible and
customizable transformations for an object-oriented language. We hope
that our results and observations will prove valuable for further
development.

\section{Analysis and results}

We define the creation of an open compiler (for Java) using Stratego
as the combined eventual goal of this project and other projects that
will follow it. Such a compiler has to differ from existing Java
compilers in several key points. Most modern Java systems resort to
just in time (JIT) compilation in order to ensure performance, which
essentially means that they use a less sophisticated compiler to
statically transform source code to bytecode and then transform
bytecode to machine code at runtime. Our compiler should achieve
performance primarily by means of static code optimizations. This does
not prevent the later use of a JIT compiler at runtime, in which case
the performance boost given by the static optimization would be
combined with that of JIT compilation.  Furthermore, we aim to differ
from other compilers by making sure that the meta programmer can
easily add her extensions to the language.  This can be achieved by
employing a MetaBorg-style model of embedding concrete syntax for
other (domain specific) languages in the base language's syntax
definition and parser. Finally, we want to open up the compiler
internals to a level that is fully manageable by a regular user so
that she can optionally intervene in the compilation process (more
specifically, aid the compiler's optimizations).

\paragraph{Control flow}

While the open compiler for Java is the final goal, we are aware that
the current point of development that the Stratego platform has
reached could not have allowed us to achieve it within the bounds of
this thesis. What we aimed to do instead was take concrete steps in
that direction. To that end, we acknowledged that support for
data-flow analysis (an essential component in an optimizing compiler,
as ours is intended to be) is still incomplete in Stratego, so one of
our goals was to contribute to improving this state of affairs.
Already achieved and proved capabilities of Stratego for doing
data-flow analysis included the possibility of peephole (restricted to
a limited number of statements) and intraprocedural (restricted to the
contents of a method) optimizations, as long as only basic control
flow (sequencing, branching and iteration) was involved. However,
there was no support for more involved control flow (including
exception handling or handling of break's or continue's to labels).
Also, before this thesis there had been no (documented) experiments
with interprocedural control-flow (i.e., code with procedure/method
calls), so a scheme for this also had to be devised.

In terms of results, strategies for supporting common non-sequential
control flow instructions (break, continue, return and exceptions)
have been successfully implemented, but they can currently support
forward propagation data-flow transformations only. We have also
implemented prototype support for backward propagation data-flow
transformations, but this is still only marginally tested and only
supports the break instruction. Regarding interprocedural control
flow, we provided a couple of examples as part of the pointer analysis
and generic typestate propagation work we discuss below. These
examples illustrate how properties can be transferred from actual to
formal arguments and from return variables to call site, how the
implicit \icode{this} pointer can be handled and how virtual dispatch
can be managed.

\paragraph{Pointer analysis}

The lack of a support for complex control flow was not the only aspect
of Stratego which impeded more realistic data-flow analyses.  Before
our work, potential implementers of data-flow analyses also had to
deal with the absence of aliasing information. Aliasing indicates
which variables, object fields or class fields point to the same
memory location, or \emph{are aliased}. Such information is
indispensable to most analyses, at least if we do not want them to be
excessively conservative. Without aliasing information, an analysis
will always have to make the conservative assumption that all
variables, object fields and class fields may point to the same
location. This means that every time an analysis sees, say, a
modification of a variable, it has to assume that any other variable
could have been changed as well (since it could be the case that the
latter variable also points the memory location that was changed). It
quickly becomes obvious that with such an assumption the effect of
most analyses will be severely limited. Therefore, another goal of
this thesis was to strengthen Stratego's support for more precise
data-flow analyses by implementing a algorithm for computing alias
information.

While this goal has been achieved for a prototype object-oriented
language, some work still has to be done in order to map the
implementation in order for it to support, e.g., the Java language.
The two core algorithms that are involved do not change at all.
However, some effort will be required to replace all the syntactic
constructs that pertain to our prototype language with the
Java-specific ones, given that a range of supporting tools are
required by the core analysis (class hierarchy information, type
information, code simplification). The pointer analysis is flow- and
context-insensitive. This has the advantage of being fast and easier
to integrate in various analyses and the disadvantage of being less
specific. Analysis results are available in a user friendly format,
but they currently require that the user perform a (provided) variable
annotation algorithm on the analyzed code, in order to ensure a unique
naming scheme for the program variables. It is improbable that this
can be avoided, but better integration with the platform can be
achieved.

\paragraph{Extensible and customizable transformations}

Our final objective for this thesis was to investigate how an existing
data-flow transformation can be modified to cover extensions of the
language (either with new language constructs or with embedded DSLs)
and how user annotations can be used to aid such a transformation
(extended or not). Support for extending and customizing
transformations prepares the ground for open compilers by already
enabling applications like active libraries and DSL embeddings. Active
libraries is a term coined by contrast to the well-known static
libraries of today. Their activeness stems from the fact that the
library is meant to become an active component of the compilation
process. The user will be given an (annotation) language which she
will use to specify compilation information and/or compilation
actions. These will be used by the compiler wherever the compilation
process encounters code that uses the library in order to produce a
user-customized result (usually for purposes of optimizing the way the
library is used). The concept of active libraries will be further
detailed in chapter \ref{ch:ectrans}. In perspective, a similar
mechanism would be used when building an open compiler for allowing
user intervention in the process.

DSL embeddings have already been introduced in the previous section in
the context of MetaBorg. Extensible transformations should come to
complete MetaBorg with a means to extend transformations along with
syntax. The general idea is that we have a general purpose language
\emph{and} a number of transformations (optimizing or otherwise)
defined to support all the constructs of the language. MetaBorg
provides a method for extending the language with additional concrete
syntax that supports domain-specific operations. What is still missing
is a method for extending the transformations defined for the base
language as well so that they support the additional constructs. Of
course, one solution is to leave the transformations as they are and
apply them only after the statements specified in the embedded DSL
have been assimilated to base language constructs, but this often
results in missing optimization opportunities due to the loss of
semantics that is inevitable in the assimilation process. Therefore,
we consider extending the transformations to cover the new syntax as
well to be a more attractive solution, since it will likely result in
better optimization.

We have only managed to cover the first part of this objective in our
thesis, namely experimenting with customizable transformations.
Experimentation with the extensibility of transformations, although
initially planned as part of our work, has not become reality, mostly
due to unforeseen difficulties along the way with the other goals,
which ended up taking more time than expected. Coming back to what has
been done, we have experimented with customizable transformations by
implementing a generic typestate propagation algorithm together with a
mechanism that allows a user to specify code changes based on
typestate information (pursuing some of the ideas of active
libraries). Both the typestate change information and the code change
information are provided by the user in an domain specific annotation
language of our own design.

\paragraph{Context}

The challenges of this thesis work have varied throughout its
different goals. While some development was essentially new and based
on little or no previous work (strategies for supporting
non-sequential control flow), some has been an adaptation to Stratego
of already existing algorithms (pointer analysis) and some has been
inspired by other work, but took a rather different form and followed
a different path (generic typestate propagation). Regardless of which
part we have in mind, the main focus of this entire thesis was on
enriching the Stratego platform with additional user support and
enriching the Stratego code base with interesting solutions that can
be the base for further work or even be used as is, perhaps with some
adaptation.  Filling in all the remaining missing pieces that will
make it possible to use Stratego for developing an open compiler will
still take a lot of effort, but nevertheless we hope to have made a
significant contribution with the work presented herein.

\section{Research questions}

Now that we have sketched the general topics that this thesis
comprises of, we can advance to defining the research questions that
have guided our work. While we are confident that some of them have
found their answer during this project, we are also aware that some
have been considered only partially and some even not at all
(particularly those related to extensible transformations). Despite
this, we list them all here as they have been defined before our work
had begun. In this way, the reader (perhaps coming back to these
questions after reading the thesis) can get a good grasp of the extent
in which we have managed to achieve our goals.

\begin{itemize}
\item \textbf{The Stratego model for data-flow analysis.} Related to
  this, we have tried to answer questions like: ``how complete is the
  support currently offered by Stratego for managing control flow?'',
  ``what is still needed to ensure proper abstractions for dealing
  with all constructs of modern object-oriented languages (and Java in
  particular)?'', ``what is the most appropriate mechanism which we
  can provide within Stratego for dealing with non-sequential control
  flow (such as exception handling or breaking and continuing)?'',
  ``can we build this mechanism in a layered way, with intermediate
  level abstractions and high level abstractions built on top of
  them?'', ``if so, what are these intermediate level abstractions?'',
  ``what is a good model for building interprocedural data-flow
  analyses in Stratego?''.
\item \textbf{Information for data-flow analyses.} Our efforts in this
  area have been guided by the following questions: ``how can we use
  the Stratego model to implement essential analyses like pointer
  analysis and escape analysis?'', ``can we compute this information
  on demand or does it have to be done a priori?'', ``what is a proper
  way to represent such information (i.e., the user has to query for,
  say, the aliases of a variable, but a variable name is not an unique
  identifier, so how do we compose a key that is natural enough for
  the user to use easily)?'', ``is the accuracy of the proposed
  context-insensitive and flow-insensitive pointer analysis good
  enough for providing a real benefit to other data-flow analyses?'',
  ``for both the pointer and the alias analysis we need a call graph:
  can we use the information provided by Dryad to build one
  efficiently?''.
\item \textbf{Extensible and customizable transformations.} The
  questions that we have focused on in our experiments with extensible
  and customizable transformation are as follows: ``how can a
  transformation be extended in a modular way in order to support new
  syntactic constructs in a language?'', ``do we need a new way in
  which to compose transformations for allowing modular extension?'',
  ``how can the user interact with transformations in particular
  cases?'', ``how can this be generalized so that virtually any
  transformation can be customized?'', ``how useful is it, in the
  context of customization, to provide a generic mechanism for
  propagating typestate?'', ``does the customization mechanism
  naturally scale to cover extended transformations as well?'', ``what
  mechanisms (except the ones covered by this thesis) are still needed
  for writing an open compiler for Java?''.
\end{itemize}

\begin{comment}
\section{Thesis preview}

The work we propose to pursue in this thesis can be structured in
three distinct layers, each with its own objectives. We give a quick
preview of these three layers here and we then discuss them at length
in the remaining sections of this proposal. A number of the tasks are
marked as secondary. This means that they will only be addressed if
time allows it. Whether this will be the case or not depends on how
work on the other (primary) ones goes.

Here are, then, the three layers and their subdivisions:

\begin{itemize}

\item extend and improve dynamic rules library by:
  \begin{itemize}
  \item \textit{writing an extensive unit test suite based on
      TIL}\footnote{TIL stands for Tiny Imperative Language, it
      represents the very thing its name suggests, and will be
      described later.}: the dynamic rules library of Stratego is only
    partially tested at this point, and tests have not kept up with
    development lately. A proper and complete test suite for this
    library is severely needed due to its increased complexity.
  \item \textit{fix library code based on unit testing results}: we
    are already aware of one bug in the dynamic rules library and we
    expect a full-fledged test suite to reveal more. A mature library
    used for data-flow analyses (or anything else, for that matter),
    is naturally expected to have all known bugs fixed.
  \item \textit{provide support for break and continue statements with
      Java labels}: the Java language allows a controlled form of the
    goto statement through labels combined with the break and continue
    statements. Currently, the dynamic rules library does not provide
    any abstractions for handling such statements elegantly.
  \item \textit{provide support for Java exceptions}: an abstraction
    for handling exceptions lacks as well.
  \item \textit{extend constant propagation to handle break, continue
      and exceptions}: this would be a proof-of-concept implementation
    of a flow sensitive data-flow analysis for a fully-fledged
    object-oriented language (Java) which is meant to validate and
    illustrate the support for break, continue and exceptions.
  \item (secondary) \textit{come up with intermediate level
      abstractions for the various operations involving dynamic
      rules}: with the addition of support for break, continue and
    exception handling, a considerable amount of duplication of
    (conceptual) operations will probably appear. It would be
    desirable to identify and extract common operations and rebuild
    existing abstractions on top of them. This would avoid duplication
    \emph{and} allow a power user to reuse the intermediate level
    abstractions in different, unforeseen contexts.
  \end{itemize}

\item implement escape \& pointer analysis:
  \begin{itemize}
  \item \textit{construct a call graph for Java programs}: this
    information is needed by both the pointer and escape analyses, but
    it can prove useful to a number of other data-flow analyses as
    well.
  \item \textit{implement pointer analysis}: as discussed, the results
    of this analysis are needed if we ever intend Stratego to be used
    for developing any other data-flow analysis effectively.  However,
    it is not only the need for the alias information itself which
    motivates this, but also the fact that we want an example of a
    reasonably complex data-flow analysis done in Stratego in order to
    validate the model in a ``real-life'' case.
  \item (secondary) \textit{implement escape analysis}: escape
    information is particularly useful for removing unneeded
    synchronization and allocating Java objects that are local to a
    method on the stack instead of the heap, so a future compiler that
    would perform these two optimizations would benefit from escape
    analysis. It would also provide further experience with using
    Stratego for complex data-flow analyses.
  \end{itemize}

\item explore customizing and extending of transformations:
  \begin{itemize}
  \item \textit{experiment with some simple cases to find out how
      transformations can be extended along with extensions to a
      language}: we will try to find a structured way in which to
    extend transformations for supporting additional syntax added to a
    language.
  \item \textit{experiment with propagation of typestates}: typestates
    represents states of variables that can be computed and propagated
    at compile time. The state information thus propagated can be used
    by the compiler to infer potential problems in the semantics of
    the program. We expect the model for user customization of
    data-flow transformations to be related to typestate propagation,
    so work on the latter should provide insight on the former.
  \item \textit{experiment with an optimization which can benefit from
      user annotations}: in order to find out how exactly a user can
    aid optimizing transformations, we plan to imagine such an
    optimization and discover where and what kind of user input is
    valuable.
  \item (secondary) \textit{create a generalized symbolic merge
      operation for typestates}: currently, Stratego allows union and
    intersection of dynamic rules to be used as merge operations after
    a two-way split in the control flow. We would like to generalize
    this to support random merge operations for multi-way split. This
    requires investigating how the current model for representing
    dynamic rules can be adapted to support this. Typestates in
    particular motivate implementing this because they usually have to
    be merged according to custom rules (i.e., not by intersection or
    union).
  \end{itemize}
\end{itemize}
\end{comment}

\section{Thesis overview}

\fix{
Write this section.
}

\begin{comment}
The rest of this thesis is organized as follows. Chapter
\ref{ch:project-setting} discusses the set up of our project,
focusing on the tools and technologies that will be employed, studied
or worked in. The following three chapters describe the three layers
(or phases) this thesis is structured on. All of them have the same
organization: an overview subsection, a related work subsection and a
proposed work subsection. Thus, chapter \ref{ch:dynam-rules-libr},
\ref{ch:pointer-analysis} and \ref{ch:ectrans}
describe our proposed work on the dynamic rules library, on pointer
and escape analysis and on extensible and customizable
transformations, respectively. We end this thesis with chapter
\ref{ch:planning}, which gives a tentative planning for the work
planned herein.
\end{comment}
